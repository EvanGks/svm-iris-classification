{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM) for Classification: A Comprehensive Project\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Support Vector Machines (SVM) are powerful supervised learning models used for classification and regression tasks. They work by finding the optimal hyperplane that separates classes in the feature space and can handle both linear and non-linear problems using kernel functions.\n",
    "\n",
    "**Applications:**\n",
    "- Text Classification (e.g., sentiment analysis, product reviews)\n",
    "- Image Classification (e.g., handwritten digit recognition, object detection)\n",
    "- Bioinformatics (e.g., protein classification, disease prediction)\n",
    "\n",
    "**Problem Statement:**\n",
    "In this project we demonstrate the end-to-end process for building an SVM-based classification model using a real-world dataset (the Iris dataset). The dataset is stored and processed in JSON format. We include sections for data exploration, preprocessing, a mathematical explanation of SVM, model training and evaluation, visual analysis of the results, and concluding discussions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_description_and_EDA",
   "metadata": {},
   "source": [
    "## Dataset Description & Exploratory Data Analysis (EDA)\n",
    "\n",
    "The Iris dataset comprises 150 samples with 4 features (sepal length, sepal width, petal length, and petal width) and 3 classes representing different iris species. In this section, we load the dataset from a JSON file, inspect its structure, and perform basic visualizations to identify key patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b5e376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b70dc9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset from scikit-learn and save it as a JSON file\n",
    "from sklearn.datasets import load_iris\n",
    "iris_sklearn = load_iris()\n",
    "\n",
    "# Convert the dataset to a dictionary format\n",
    "iris_dict = {\n",
    "    'data': iris_sklearn.data.tolist(),\n",
    "    'target': iris_sklearn.target.tolist(),\n",
    "    'feature_names': iris_sklearn.feature_names,\n",
    "    'target_names': iris_sklearn.target_names.tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37280955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset to a JSON file\n",
    "with open('iris.json', 'w') as f:\n",
    "    json.dump(iris_dict, f, indent=4)\n",
    "\n",
    "# Load the dataset from the JSON file\n",
    "with open('iris.json', 'r') as f:\n",
    "    iris_json = json.load(f)\n",
    "\n",
    "# Convert the JSON data to a pandas DataFrame\n",
    "df = pd.DataFrame(iris_json['data'], columns=iris_json['feature_names'])\n",
    "df['target'] = iris_json['target']\n",
    "\n",
    "# Map target to species names for clarity\n",
    "target_map = {idx: name for idx, name in enumerate(iris_json['target_names'])}\n",
    "df['species'] = df['target'].map(target_map)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EDA_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic EDA\n",
    "print(\"DataFrame Shape:\", df.shape)\n",
    "print(\"\\nData Types:\\n\\n\", df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5dc65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStatistical Description:\\n\\n\", df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b052c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize relationships between features using seaborn's pairplot\n",
    "sns.pairplot(df, hue='species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb30c48b",
   "metadata": {},
   "source": [
    "### Interpretation of Pairplot\n",
    "\n",
    "The pairplot above reveals that the features **petal length (cm)** and **petal width (cm)** provide the clearest separation between the three iris species. Setosa is linearly separable from the other two classes in almost all feature combinations, while Versicolor and Virginica overlap more, especially in sepal measurements. This insight guides our later choice to visualize decision boundaries using petal features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_preprocessing",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this section, we handle any missing values (if present), apply feature scaling, encode categorical variables (if applicable), and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df[iris_json['feature_names']]\n",
    "y = df['target']\n",
    "\n",
    "# Scale features using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Print class distribution in train and test sets to verify stratification\n",
    "print(\"Class distribution in the full dataset:\", collections.Counter(y))\n",
    "print(\"Class distribution in the training set:\", collections.Counter(y_train))\n",
    "print(\"Class distribution in the test set:\", collections.Counter(y_test))\n",
    "\n",
    "# Output shapes of the training and testing sets\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8133aedf",
   "metadata": {},
   "source": [
    "> **Why Feature Scaling?**  \n",
    "Support Vector Machines are sensitive to the scale of input features because the algorithm relies on distance calculations to find the optimal hyperplane. Features with larger scales can dominate the distance metric, leading to suboptimal decision boundaries. Standardizing features ensures that each feature contributes equally to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical_explanation",
   "metadata": {},
   "source": [
    "## Mathematical Explanation of SVM\n",
    "\n",
    "Support Vector Machines aim to determine the optimal hyperplane that separates different classes in the feature space. Key concepts include:\n",
    "\n",
    "1. **Hyperplane Separation:**\n",
    "   - A hyperplane is a decision boundary. In a 2D space it is a line, while in higher dimensions it generalizes to a plane or hyperplane.\n",
    "\n",
    "2. **Maximizing the Margin:**\n",
    "   - The margin is the distance from the hyperplane to the nearest data points (support vectors). SVM maximizes this margin to improve model generalization.\n",
    "\n",
    "3. **Kernel Functions:**\n",
    "   - Kernel functions (e.g., linear, polynomial, radial basis function) allow SVMs to perform non-linear classification by mapping data to a higher-dimensional space where a linear separation is feasible.\n",
    "\n",
    "4. **Optimization Techniques:**\n",
    "   - The SVM training process involves solving a convex quadratic programming problem, ensuring that the global optimum is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600eac5f",
   "metadata": {},
   "source": [
    "## SVM Optimization Objective\n",
    "\n",
    "The SVM optimization problem can be formulated as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\underset{\\mathbf{w}, b}{\\text{minimize}} \\quad \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\\\\n",
    "& \\text{subject to} \\quad y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1, \\quad \\forall i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{w}$ is the weight vector, $b$ is the bias, and $y_i$ are the class labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b72ca9",
   "metadata": {},
   "source": [
    "### Visual Explanation: Margin and Support Vectors\n",
    "\n",
    "Below is a simple diagram illustrating the SVM concept of margin and support vectors:\n",
    "\n",
    "![SVM Margin and Support Vectors](https://miro.medium.com/v2/resize:fit:1400/1*oRk-5aab0G8SkBX2fpw8Gw.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_training_evaluation",
   "metadata": {},
   "source": [
    "## Model Training & Evaluation\n",
    "\n",
    "We now train an SVM model using scikit-learn's SVC, perform hyperparameter tuning via grid search, and evaluate the model's performance with metrics such as accuracy, classification report, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set up a parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Initialize the Support Vector Classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display the best parameters found\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "best_svc = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_svc.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_analysis_visualization",
   "metadata": {},
   "source": [
    "## Model Analysis & Visualization\n",
    "\n",
    "Although the Iris dataset has 4 features, we can visualize decision boundaries by considering the two most discriminative features: **petal length** and **petal width**. We also display a heatmap of the confusion matrix for the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "visualize_decision_boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualization, select two features: petal length and petal width\n",
    "features = ['petal length (cm)', 'petal width (cm)']\n",
    "X_vis = df[features]\n",
    "y_vis = df['target']\n",
    "\n",
    "# Scale the two selected features\n",
    "X_vis_scaled = scaler.fit_transform(X_vis)\n",
    "\n",
    "# Train an SVM on these two features using the best parameters determined earlier\n",
    "svc_vis = SVC(C=grid_search.best_params_['C'], \n",
    "              kernel=grid_search.best_params_['kernel'], \n",
    "              gamma=grid_search.best_params_['gamma'])\n",
    "svc_vis.fit(X_vis_scaled, y_vis)\n",
    "\n",
    "# Create a mesh to plot the decision boundaries\n",
    "x_min, x_max = X_vis_scaled[:, 0].min() - 1, X_vis_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_vis_scaled[:, 1].min() - 1, X_vis_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                     np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "# Predict over the grid\n",
    "Z = svc_vis.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a90826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary and support vectors\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
    "sns.scatterplot(x=X_vis_scaled[:, 0], y=X_vis_scaled[:, 1], \n",
    "                hue=[target_map[val] for val in y_vis], palette='coolwarm', edgecolor='k')\n",
    "# Plot support vectors\n",
    "plt.scatter(svc_vis.support_vectors_[:, 0], svc_vis.support_vectors_[:, 1], \n",
    "            s=120, facecolors='none', edgecolors='black', linewidths=1.5, label='Support Vectors')\n",
    "plt.xlabel(features[0])\n",
    "plt.ylabel(features[1])\n",
    "plt.title('SVM Decision Boundary (Using Petal Dimensions)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba854d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for the full model trained on all features\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris_json['target_names'], \n",
    "            yticklabels=iris_json['target_names'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82504e44",
   "metadata": {},
   "source": [
    "### Confusion Matrix Interpretation\n",
    "\n",
    "The confusion matrix above shows the number of correct and incorrect predictions for each class. For example, if the model misclassifies a sample of *Iris-versicolor* as *Iris-virginica*, it will appear in the corresponding cell. In this run, the model achieves high accuracy, with very few (if any) misclassifications, indicating strong performance on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discussion",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The SVM model achieved high accuracy on the Iris dataset, confirming its effectiveness for classification tasks on even small datasets. Key observations include:\n",
    "\n",
    "- **Decision Boundary:** The two-feature visualization shows well-separated classes, indicating that petal dimensions are highly discriminative.\n",
    "- **Hyperparameter Tuning:** The grid search helped select optimal parameters improving performance.\n",
    "- **Model Limitations:** While SVM performs well on this dataset, larger or more complex datasets may require more computational resources and careful kernel selection.\n",
    "\n",
    "Future comparisons with other models (e.g., Decision Trees, K-Nearest Neighbors) can provide insight into alternative approaches for similar classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project demonstrated a comprehensive machine learning pipeline using SVM for classification. Key steps included:\n",
    "\n",
    "- Loading and processing data stored in JSON format\n",
    "- Conducting Exploratory Data Analysis (EDA)\n",
    "- Data preprocessing and feature scaling\n",
    "- Providing a mathematical explanation of the SVM algorithm\n",
    "- Training the SVM model with hyperparameter tuning and evaluating its performance\n",
    "- Visualizing decision boundaries and the confusion matrix\n",
    "\n",
    "Future work could extend this approach to larger, more complex datasets and explore alternative kernel functions or classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "references",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Cortes, C. and Vapnik, V. (1995). Support-vector networks. *Machine Learning*, 20(3), 273-297.\n",
    "2. [scikit-learn: Machine Learning in Python](https://scikit-learn.org/stable/)\n",
    "3. Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.\n",
    "4. [The Iris Dataset: Fisher's Iris Data](http://archive.ics.uci.edu/ml/datasets/Iris)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
